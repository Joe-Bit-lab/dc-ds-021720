{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mod 3 CRISP-DM Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "By the end of this lab you will be able to:\n",
    "- Understand the steps of the CRISP-DM model\n",
    "- Apply the steps of the CRISP-DM model to a business problem using mod 3 skillsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRoss-Industry Standard Process for Data Mining (CRISP-DM)\n",
    "\n",
    "Before we dig into the problem, lets refresh our memories on the steps in the CRISP-DM model.\n",
    "\n",
    "<img src=\"img/new_crisp-dm.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data\n",
    "\n",
    "<img src=\"img/grocery-cart.jpg\" width=\"500\">\n",
    "\n",
    "The data scientists at BigMart have collected 2013 sales data for 1559 products across 10 stores in different cities.\n",
    "\n",
    "The data is located in the csv called `big_mart.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We previously explored the features of the big_mart dataset but now BigMart wants us to answer the following question:\n",
    "\n",
    "**The sales team at BigMart wants to know which products have high sales.  They ask you to help them  predict if sales of each product at a particular store will be high or low using the big_mart dataset.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Data Understanding\n",
    "\n",
    "We have already done a great deal of exploratory data analysis of this dataset.  Let's refresh out memory on what the original data contained. \n",
    "\n",
    "<img src=\"img/big_mart_data_variables.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Data Preparation\n",
    "\n",
    "This step has already been done for you.  The following steps were taken and are reflected in the `big_mart_clean.csv` file.\n",
    "\n",
    "- Imputed missing values for `Outlet_Size` (replaced missing with the mode)\n",
    "- Imputed missing values for `Item_Weight` (replaced missing with the average)\n",
    "- Created a dichotomous variable `Item_Sales_Cat` for high vs low sales (low sales are defined as less than 2000)\n",
    "- Cleaned typos in `Item_Fat_Content`\n",
    "- Created new variable `Item_Type_Combined` (labels items as food, non-consumable, or drinks)\n",
    "- Created new variable `Outlet_Years` (the years of operation of a store)\n",
    "\n",
    "\n",
    "#### Before we begin modeling let's do a quick exploration of out new dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explore the dataset\n",
    "big_mart = pd.read_csv('big_mart_clean.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_mart.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_mart.Item_Identifier.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's look at the target variable (Item_Sales_Cat) a little more in depth to examine the classes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Modeling\n",
    "\n",
    "Once we have clean data, we can begin modeling! Remember, modeling, as with any of these other steps, is an iterative process. During this stage, we'll try to build and tune models to get the highest performance possible on our task.\n",
    "\n",
    "Consider the following questions during the modeling step:\n",
    "\n",
    "- Is this a classification task? A regression task? Something else?\n",
    "- What models will we try?\n",
    "- How do we deal with overfitting?\n",
    "- Do we need to use regularization or not?\n",
    "- What sort of validation strategy will we be using to check that our model works well on unseen data?\n",
    "- What loss functions will we use?\n",
    "- What threshold of performance do we consider as successful?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/talking.jpeg\" width=\"60\" align='left'>\n",
    "\n",
    "#### Turn and Talk\n",
    "\n",
    "How would we answer these modeling questions in order to answer the business question we established above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preparation\n",
    "\n",
    "Before we begin modeling let's split our data and then perform encoding/scaling steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set random state for our notebook\n",
    "import numpy as np\n",
    "np.random.seed(217)\n",
    "\n",
    "y = big_mart['Item_Sales_Cat']\n",
    "X = big_mart.drop(columns=['Item_Sales_Cat', 'Item_Identifier'])\n",
    "\n",
    "#split data into test and train sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get shape of the training and test sets\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add code here\n",
    "numeric_features = ['Item_Weight', 'Item_Visibility',\n",
    "                     'Item_MRP', 'Outlet_Years']\n",
    "ss=StandardScaler()\n",
    "\n",
    "categorical_features = [ 'Item_Fat_Content',\n",
    "       'Item_Type', 'Outlet_Identifier',\n",
    "       'Outlet_Size', 'Outlet_Location_Type',\n",
    "       'Outlet_Type', 'Item_Type_Combined' ]\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"ordinal\", OrdinalEncoder()),\n",
    "    ('onehot', OneHotEncoder(drop='first'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', ss, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)], remainder='passthrough')\n",
    "\n",
    "#let's use it now!\n",
    "\n",
    "X_train_tran = preprocessor.fit_transform(X_train)\n",
    "X_train_tran\n",
    "\n",
    "#and we can transform X_test too\n",
    "\n",
    "X_test_tran = preprocessor.transform(X_test)\n",
    "X_test_tran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get column names for categories\n",
    "cat_names = preprocessor.named_transformers_['cat'].named_steps['ordinal'].categories_\n",
    "\n",
    "cat_names = [val for sublist in cat_names for val in sublist[1:]]\n",
    "cat_names\n",
    "\n",
    "#full list of column names\n",
    "column_names = numeric_features + cat_names\n",
    "\n",
    "#apply column names to dataframe\n",
    "X_train_trans = pd.DataFrame.sparse.from_spmatrix(X_train_tran, columns=column_names)\n",
    "X_train_trans.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dummy Model\n",
    "\n",
    "First we are going to start with a dummy model to predict if the product has high or low sales. In our dummy model we classify everything as the majority class.\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DummyClassifier to predict only target 0\n",
    "dummy = DummyClassifier(strategy='most_frequent')\n",
    "dummy.fit(X_train_tran, y_train)\n",
    "dummy.score(X_test_tran, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/talking.jpeg\" width=\"60\" align='left'>\n",
    "\n",
    "#### Turn and Talk\n",
    "\n",
    "What does the score from this model tell us?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have run our dummy model let's create a nice looking confusion matrix using the [yellowbrick package](https://www.scikit-yb.org/en/latest/api/classifier/confusion_matrix.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = ConfusionMatrix(dummy)\n",
    "\n",
    "# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "# and then creates the confusion_matrix from scikit-learn.\n",
    "cm.score(X_test_tran, y_test)\n",
    "\n",
    "# How did we do?\n",
    "cm.poof()\n",
    "\n",
    "#specify the target classes\n",
    "#classes = [\"unoccupied\", \"occupied\"]\n",
    "\n",
    "# Instantiate the classification model and visualizer\n",
    "y_pred=dummy.predict(X_test_tran)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Vanilla\" Logistic Regression Model\n",
    "\n",
    "Now that we know what our dummy/baseline classifier does let's fit a \"vanilla\" (aka, with all the defaults) Logistic Regression Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Vanilla\" KNN Model\n",
    "\n",
    "OK, now we know how the Logistic Regression model works, but maybe there is a model that does even better.  Let's try a KNN Model next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Evalution\n",
    "\n",
    "During the evaluation step we want to evaluate the results of our models, and decide the next steps in selecting the \"best\" model.  During this step we should consider the following:\n",
    "\n",
    "- Does our model solve the business problem?\n",
    "- What metrics should we be using to evaluate the \"success\" of our model?\n",
    "- Can we further improve our models?\n",
    "- Do we need more data?  Or different data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/talking.jpeg\" width=\"60\" align='left'>\n",
    "\n",
    "#### Turn and Talk\n",
    "\n",
    "What model do you think is \"best\" and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
