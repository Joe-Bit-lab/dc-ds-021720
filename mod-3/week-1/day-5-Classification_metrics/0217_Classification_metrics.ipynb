{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating and Tuning a Binary Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals\n",
    "\n",
    "After this lesson, you should be able to:\n",
    "\n",
    "- Build and explain confusion matrices from a model output\n",
    "- Calculate various binary classification metrics\n",
    "- Explain the AUC/ROC curve, why it matters, and how to use it\n",
    "- Understand when and how to optimize a model for various metrics\n",
    "- Optimize a classification model based on costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heart Disease Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Dataset info](https://archive.ics.uci.edu/ml/datasets/Heart+Disease)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/heart.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our dataset we have 303 patients and 13 independent variables and 1 binary target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we are working with classification problems it is always good practice to check the class balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that approximately %54 of the patients are in the class 0 which refers to 'no presence' of a heart disease. Consequently, %45 of the patients have a heart disease. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For model evaluation we split our data into two parts: Train - Test\n",
    "\n",
    "X = df.drop('target', axis = 1)\n",
    "y = df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    random_state = 77, \n",
    "                                                    stratify = y, # in classification problems \n",
    "                                                                  # when you split the data \n",
    "                                                                  # you want to keep the ratio in the classes.\n",
    "                                                    test_size = .2 # This is usually the ratio but it might change \n",
    "                                                                   # according to the problem at hand.\n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's check number of 1 and 0 in both datasets\n",
    "y_train.mean(), y_test.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!! Now __forget__ the test set\n",
    "\n",
    "[sklearn - Why do we need Train-Test-Validation?](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing a Perfomance Metric for Model Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Model Selection vs Model Evaluation__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model Selection/Model Comparison: What is the best parameters for a given model. Between different models which one is better models the reality.\n",
    "\n",
    "Ex: If we are working with an app that runs a machine learning algorithm model selection is choosing the process of choosing a final algorithm to deploy.\n",
    "\n",
    "\n",
    "- Model Evaluation: After selecting a 'best' model with model selection how this model will perform in the 'real' case.\n",
    "\n",
    "Ex: Model evaluation is where we want to predict how successful this algorithm will be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Available tools in sklearn](https://scikit-learn.org/stable/model_selection.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src= 'images/table.png' width = 450 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "\n",
    "$$\\text{Accuracy} =  \\frac{\\text{# of Correct Predictions}}{\\text{# of Total Cases}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Accuracy overall gives a good idea about an estimators performance but sometimes it is not directly relevant to the problem. (Especially in imbalanced dataset we should expect that event the dummy model could perform a high accuracy.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall\n",
    "\n",
    "$$ \\text{Recall} = \\frac{\\text{# True Positives}}{\\text{# of Condition Positive}} = \\frac{\\text{TP}}{\\text{TP + FN}} $$\n",
    "\n",
    "- __Q__: Given that the total number of \"Condition Positives\" are fixed. How can we improve the __Recall__ score?\n",
    "\n",
    "\n",
    "- In our case, recall score corresponds to out of 100 patients with heart disease how many of them are succesfully predicted as positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision\n",
    "\n",
    "$$ \\text{Precision} = \\frac{\\text{# True Positives}}{\\text{# of Predicted Positive}} = \\frac{\\text{TP}}{\\text{TP + FP}} $$\n",
    "\n",
    "- __Q__: Given that the total number of \"Condition Positives\" are fixed. How can we improve the __Precision__ score?\n",
    "\n",
    "- In our case, precision score corresponds to: out of 100 positive prediction how many of them are really the having heart disease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your turn__\n",
    "\n",
    "- Suppose we are trying to classify videos whether they are safe for kids or not. Which metric does make more sense to use? (safe = 1, not_safe = 0)\n",
    "\n",
    "- We are training a classification algorithm for fraud detection for a bank. Which metric does make more sense to use? (fraud = 1, normal = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep Before Training a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[A good blog post on handling categorical variables](https://www.bogotobogo.com/python/scikit-learn/scikit_machine_learning_Data_Preprocessing-Missing-Data-Categorical-Data.php)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# we can also check the categorical variables with scatter matrix plot\n",
    "# but notice that this is not practical in higher dimensions\n",
    "pd.plotting.scatter_matrix(df, figsize= (14, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_variables = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_list = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[There are many interesting tools for processing data](https://scikit-learn.org/stable/modules/generated/sklearn.compose.make_column_selector.html#sklearn.compose.make_column_selector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your Turn__\n",
    "\n",
    "- Convert Categorical Variables to OneHotEncoding\n",
    "\n",
    "- [Dummies vs OneHot: Read the second answer](https://stackoverflow.com/questions/36631163/pandas-get-dummies-vs-sklearns-onehotencoder-what-are-the-pros-and-cons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(X_train, columns= categorical_variables, drop_first= True).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try to transform test data with get_dummies method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create an encoder object. This will help us to convert\n",
    "## categorical variables to new columns\n",
    "encoder = OneHotEncoder(handle_unknown= 'error',\n",
    "                        drop='first',\n",
    "                        categories= 'auto')\n",
    "\n",
    "## Create an columntransformer object.\n",
    "## This will help us to merge transformed columns\n",
    "## with the rest of the dataset.\n",
    "\n",
    "ct = ColumnTransformer(transformers =[('ohe', encoder, ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal'])],\n",
    "                                    remainder= 'passthrough')\n",
    "ct.fit_transform(X_train)\n",
    "X = ct.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct.transformers_[0][1].get_feature_names(categorical_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:5, :6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try to transform test dataset by using ct object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Don't forget!!__\n",
    "\n",
    "- Apply the same transformations to the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest  = ct.transform(X_test)\n",
    "Xtest.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Scaling Features__ \n",
    "\n",
    "-- Let's go back to the column transformer.\n",
    "\n",
    "[Different Scalers and Their Effect on Data](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_scaler = StandardScaler()\n",
    "standard_scaler.fit(X)\n",
    "X = standard_scaler.transform(X)\n",
    "## apply the trained transformations to test.\n",
    "\n",
    "Xtest = standard_scaler.transform(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(X,axis = 0)\n",
    "\n",
    "## What do you expect if you check the means of X_test? Try"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Check sklearn for documentation of Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "\n",
    "\n",
    "[For solvers](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(penalty = 'none', max_iter= 10000)\n",
    "log_reg.fit(X, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## What is this score?\n",
    "print(log_reg.score(X, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = log_reg.predict(X)\n",
    "\n",
    "score = log_reg.score(X, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,9))\n",
    "sns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Pastel1');\n",
    "plt.ylabel('Actual label');\n",
    "plt.xlabel('Predicted label');\n",
    "all_sample_title = 'Accuracy Score: {0}'.format(score)\n",
    "plt.title(all_sample_title, size = 15);\n",
    "plt.savefig('toy_Digits_ConfusionSeabornCodementor.png')\n",
    "#plt.show();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your Turn__\n",
    "\n",
    "- Find Recall and Precision scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Reminder__\n",
    "\n",
    "$$ \\text{True Positive Rate} = \\text{Recall} = \\frac{\\text{# True Positives}}{\\text{# of Condition Positive}} = \\frac{\\text{TP}}{\\text{TP + FN}} $$\n",
    "\n",
    "$$ \\text{Precision} = \\frac{\\text{# True Positives}}{\\text{# of Predicted Positive}} = \\frac{\\text{TP}}{\\text{TP + FP}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## find them here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using sklearn for precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## there are other important metrics too\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Cross Validation Scores for Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(penalty = 'none', max_iter= 10000 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores = cross_val_score(log_reg, X, y_train, cv = 5, scoring= 'f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curves for Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/conf_matrix_classification_metrics.png' width=650/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_vanilla = LogisticRegression(penalty= 'none', max_iter= 10000)\n",
    "\n",
    "log_reg_l2 = LogisticRegression(penalty = 'l2', C = 0.01, max_iter= 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_vanilla.fit(X, y_train)\n",
    "\n",
    "y_probs_vanilla = log_reg_vanilla.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_l2.fit(X, y_train)\n",
    "y_probs_l2 = log_reg_l2.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## let's change the treshold to see the effect of it on FPR and TPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = []\n",
    "for item in log_reg_vanilla.predict_proba(X):\n",
    "    if item[1] <= .20:\n",
    "        predicts.append(0)\n",
    "    else:\n",
    "        predicts.append(1)\n",
    "        \n",
    "conf_matrix = pd.DataFrame(confusion_matrix(y_train, predicts),\n",
    "                           index = ['actual 0', 'actual 1'], \n",
    "                           columns = ['predicted 0', 'predicted 1'])\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting ROC curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_v, tpr_v, thresholds_v = roc_curve(y_train, y_probs_vanilla[:,1])\n",
    "fpr_l2, tpr_l2, thresholds_l2 = roc_curve(y_train, y_probs_l2[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(fpr, tpr, label = None):\n",
    "    plt.plot(fpr, tpr, linewidth =2 , label = label)\n",
    "    plt.plot([0,1], [0,1], 'k--')\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    \n",
    "plot_roc_curve(fpr_v, tpr_v, label = 'Vanilla')\n",
    "plot_roc_curve(fpr_l2, tpr_l2, label = 'L2-Penalty')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also we can measure the __A__rea __U__nder __C__urve scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for \n",
    "roc_auc_score(y_train, y_probs_vanilla[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_train, y_probs_l2[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Default Measure (in most prebuilt models) - Accuracy\n",
    "\n",
    "$$ \\frac{(TP + TN)}{(TP + FP + TN + FN)} $$\n",
    "\n",
    "<img src='./images/conf_matrix_classification_metrics.png' width=650/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Category definitions - possible outcomes in binary classification\n",
    "\n",
    "- TP = True Positive (class 1 correctly classified as class 1) - e.g. Patient with cancer tests positive for cancer\n",
    "- TN = True Negative (class 0 correctly classified as class 0) - e.g. Patient without cancer tests negative for cancer\n",
    "- FP = False Positive (class 0 incorrectly classified as class 1) - e.g. Patient without cancer tests positive for cancer\n",
    "- FN = False Negative (class 1 incorrectly classified as class 0) - e.g. Patient with cancer tests negative for cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $$ \\text{Possible misclassifications} $$\n",
    "\n",
    "<img src='./images/type-1-type-2.jpg' width=400/>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that Logistic Regression gives probability predictions for each class, in addition to the final classification. By default, threshold for the prediction is set to 0.5, but we can adjust that threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = []\n",
    "for item in log_reg_vanilla.predict_proba(X):\n",
    "    if item[1] <= .20:\n",
    "        predicts.append(0)\n",
    "    else:\n",
    "        predicts.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = pd.DataFrame(confusion_matrix(y_train, predicts),\n",
    "                           index = ['actual 0', 'actual 1'], \n",
    "                           columns = ['predicted 0', 'predicted 1'])\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The AUC / ROC curve (Area Under Curve of the Receiver Operating Characteristic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/pop-curve.png' width=500/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = X_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df['probabilities'] = log_reg_vanilla.predict_proba(X)[:, 1]\n",
    "results_df['target'] = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
