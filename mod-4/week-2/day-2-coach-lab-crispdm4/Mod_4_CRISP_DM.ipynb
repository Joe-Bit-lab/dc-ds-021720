{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mod 4 CRISP-DM Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "By the end of this lab you will be able to:\n",
    "- Understand the steps of the CRISP-DM model\n",
    "- Apply the steps of the CRISP-DM model to a business problem using hyperparameter-tuned tree-based classification models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRoss-Industry Standard Process for Data Mining (CRISP-DM)\n",
    "\n",
    "Before we dig into the problem, lets refresh our memories on the steps in the CRISP-DM model.\n",
    "\n",
    "<img src=\"img/new_crisp-dm.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data\n",
    "\n",
    "<img src=\"img/grocery-cart.jpg\" width=\"500\">\n",
    "\n",
    "The data scientists at BigMart have collected 2013 sales data for 1559 products across 10 stores in different cities.\n",
    "\n",
    "The data is located in the csv called `big_mart.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We previously explored the features of the big_mart dataset but now BigMart wants us to answer the following question:\n",
    "\n",
    "**The sales team at BigMart wants to hire someone to build a machine learning algorithm to predict whether products they sell will have high sales.  They have opened the dataset to competing consulting firms for their contract.  The winning firm will be the firm with the machine learning model which best minimizes the false positives in the _high_ class.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Data Understanding\n",
    "\n",
    "We have already done a great deal of exploratory data analysis of this dataset.  Let's refresh out memory on what the original data contained. \n",
    "\n",
    "<img src=\"img/big_mart_data_variables.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Data Preparation\n",
    "\n",
    "This step has already been done for you.  The following steps were taken and are reflected in the `big_mart_clean.csv` file.\n",
    "\n",
    "- Imputed missing values for `Outlet_Size` (replaced missing with the mode)\n",
    "- Imputed missing values for `Item_Weight` (replaced missing with the average)\n",
    "- Created a dichotomous variable `Item_Sales_Cat` for high (greater than 2500 in  sales), medium (1500-2499), or low (less than 1500 in sales)\n",
    "- Cleaned typos in `Item_Fat_Content`\n",
    "- Created new variable `Item_Type_Combined` (labels items as food, non-consumable, or drinks)\n",
    "- Created new variable `Outlet_Years` (the years of operation of a store)\n",
    "\n",
    "\n",
    "#### Before we begin modeling let's do a quick exploration of out new dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explore the dataset\n",
    "big_mart = pd.read_csv('big_mart_clean_3_classes.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_mart.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's look at the target variable (Item_Sales_Cat) a little more in depth to examine the classes\n",
    "big_mart.Item_Sales_Cat.value_counts().plot(kind='bar')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's also look at a pairplot of our dataset \n",
    "sns.pairplot(big_mart)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Modeling\n",
    "\n",
    "In an effort to become more \"green\" BigMart swears by tree-based models.  They are only giving the contract to the firm which creates the \"best\" tree-based model for them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/talking.jpeg\" width=\"60\" align='left'>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "#### Turn and Talk\n",
    "- Given BigMart's \"green\" initiative, which models can our firm attempt to use?\n",
    "- What hyperparameters are associated with each of those models?\n",
    "- What are the weaknesses of each of these models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preparation\n",
    "\n",
    "Before we begin modeling let's split our data and then perform encoding/scaling steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set random state for our notebook\n",
    "import numpy as np\n",
    "np.random.seed(217)\n",
    "\n",
    "y = big_mart['Item_Sales_Cat']\n",
    "X = big_mart.drop(columns=['Item_Sales_Cat', 'Item_Identifier'])\n",
    "\n",
    "#split data into test and train sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get shape of the training and test sets\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add code here\n",
    "numeric_features = ['Item_Weight', 'Item_Visibility',\n",
    "                     'Item_MRP', 'Outlet_Years']\n",
    "ss=StandardScaler()\n",
    "\n",
    "categorical_features = [ 'Item_Fat_Content',\n",
    "       'Item_Type', 'Outlet_Identifier',\n",
    "       'Outlet_Size', 'Outlet_Location_Type',\n",
    "       'Outlet_Type', 'Item_Type_Combined' ]\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"ordinal\", OrdinalEncoder()),\n",
    "    ('onehot', OneHotEncoder())])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', ss, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)], remainder='passthrough')\n",
    "\n",
    "#let's use it now!\n",
    "\n",
    "X_train_tran = preprocessor.fit_transform(X_train)\n",
    "X_train_tran\n",
    "\n",
    "#and we can transform X_test too\n",
    "\n",
    "X_test_tran = preprocessor.transform(X_test)\n",
    "X_test_tran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get column names for categories\n",
    "cat_names = preprocessor.named_transformers_['cat'].named_steps['ordinal'].categories_\n",
    "\n",
    "cat_names = [val for sublist in cat_names for val in sublist]\n",
    "cat_names\n",
    "\n",
    "#full list of column names\n",
    "column_names = numeric_features + cat_names\n",
    "\n",
    "#apply column names to dataframe\n",
    "X_train_trans = pd.DataFrame.sparse.from_spmatrix(X_train_tran, columns=column_names)\n",
    "X_train_trans.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dummy Model\n",
    "\n",
    "First we are going to start with a dummy model to predict if the product has high or low sales. In our dummy model we classify everything as the majority class.\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DummyClassifier to predict only target 0\n",
    "dummy = DummyClassifier(strategy='most_frequent')\n",
    "dummy.fit(X_train_tran, y_train)\n",
    "dummy.score(X_train_tran, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/talking.jpeg\" width=\"60\" align='left'>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "#### Turn and Talk\n",
    "\n",
    "- What does the score from this model tell us?\n",
    "- What additional metrics do we want to examine?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have run our dummy model let's create a nice looking confusion matrix using the [yellowbrick package](https://www.scikit-yb.org/en/latest/api/classifier/confusion_matrix.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = ConfusionMatrix(dummy)\n",
    "\n",
    "# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "# and then creates the confusion_matrix from scikit-learn.\n",
    "cm.score(X_train_tran, y_train)\n",
    "\n",
    "# How did we do?\n",
    "cm.poof()\n",
    "\n",
    "#specify the target classes\n",
    "#classes = [\"unoccupied\", \"occupied\"]\n",
    "\n",
    "# Instantiate the classification model and visualizer\n",
    "y_pred=dummy.predict(X_train_tran)\n",
    "print(classification_report(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Vanilla\" Decision Tree\n",
    "\n",
    "Now that we know what our dummy/baseline classifier does let's fit a \"vanilla\" Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Evalution\n",
    "\n",
    "During the evaluation step we want to evaluate the results of our models, and decide the next steps in selecting the \"best\" model.  During this step we should consider the following:\n",
    "\n",
    "- Does our model solve the business problem?\n",
    "- What metrics should we be using to evaluate the \"success\" of our model?\n",
    "- Can we further improve our models?\n",
    "- Do we need more data?  Or different data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Your Turn!!\n",
    "\n",
    "Using any other tree based model and hyperparameter tuning, can your group come up with the best model for predicting the items with high sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  OK now let's test out your best models and evaluate it with the test data to see who has the \"best\" model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = #your code for best model here\n",
    "best_model.fit(X_train_tran, y_train)\n",
    "best_model.score(X_test_tran, y_test)\n",
    "\n",
    "\n",
    "cm = ConfusionMatrix(best_model)\n",
    "\n",
    "# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "# and then creates the confusion_matrix from scikit-learn.\n",
    "cm.score(X_test_tran, y_test)\n",
    "\n",
    "# How did we do?\n",
    "cm.poof()\n",
    "\n",
    "#specify the target classes\n",
    "#classes = [\"unoccupied\", \"occupied\"]\n",
    "\n",
    "# Instantiate the classification model and visualizer\n",
    "y_pred=best_mosel.predict(X_train_tran)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
